apiVersion: v1
items:
- metadata:
    creationTimestamp: "2022-11-10T18:02:20Z"
    generateName: local-path-provisioner-5b5579c644-
    labels:
      app: local-path-provisioner
      pod-template-hash: 5b5579c644
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:generateName: {}
          f:labels:
            .: {}
            f:app: {}
            f:pod-template-hash: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"c995d1b8-8b3f-4ee3-989b-9068a999138e"}: {}
        f:spec:
          f:containers:
            k:{"name":"local-path-provisioner"}:
              .: {}
              f:command: {}
              f:env:
                .: {}
                k:{"name":"POD_NAMESPACE"}:
                  .: {}
                  f:name: {}
                  f:valueFrom:
                    .: {}
                    f:fieldRef: {}
              f:image: {}
              f:imagePullPolicy: {}
              f:name: {}
              f:resources: {}
              f:terminationMessagePath: {}
              f:terminationMessagePolicy: {}
              f:volumeMounts:
                .: {}
                k:{"mountPath":"/etc/config/"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
          f:dnsPolicy: {}
          f:enableServiceLinks: {}
          f:priorityClassName: {}
          f:restartPolicy: {}
          f:schedulerName: {}
          f:securityContext: {}
          f:serviceAccount: {}
          f:serviceAccountName: {}
          f:terminationGracePeriodSeconds: {}
          f:tolerations: {}
          f:volumes:
            .: {}
            k:{"name":"config-volume"}:
              .: {}
              f:configMap:
                .: {}
                f:defaultMode: {}
                f:name: {}
              f:name: {}
      manager: k3s
      operation: Update
      time: "2022-11-10T18:02:20Z"
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          f:conditions:
            .: {}
            k:{"type":"ContainersReady"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:message: {}
              f:reason: {}
              f:status: {}
              f:type: {}
            k:{"type":"Initialized"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
            k:{"type":"PodScheduled"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:message: {}
              f:reason: {}
              f:status: {}
              f:type: {}
            k:{"type":"Ready"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:message: {}
              f:reason: {}
              f:status: {}
              f:type: {}
          f:containerStatuses: {}
          f:hostIP: {}
          f:startTime: {}
      manager: k3s
      operation: Update
      subresource: status
      time: "2022-11-10T18:08:23Z"
    name: local-path-provisioner-5b5579c644-vwmfx
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: local-path-provisioner-5b5579c644
      uid: c995d1b8-8b3f-4ee3-989b-9068a999138e
    resourceVersion: "694"
    uid: 32dfa701-7a5a-47aa-8ace-eb5ecd38f62d
  spec:
    containers:
    - command:
      - local-path-provisioner
      - start
      - --config
      - /etc/config/config.json
      env:
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: rancher/local-path-provisioner:v0.0.21
      imagePullPolicy: IfNotPresent
      name: local-path-provisioner
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/config/
        name: config-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-gn67q
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: lima-k3s-1
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: local-path-provisioner-service-account
    serviceAccountName: local-path-provisioner-service-account
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: local-path-config
      name: config-volume
    - name: kube-api-access-gn67q
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-11-10T18:08:23Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-11-10T18:08:23Z"
      message: 'containers with unready status: [local-path-provisioner]'
      reason: ContainersNotReady
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-11-10T18:08:23Z"
      message: 'containers with unready status: [local-path-provisioner]'
      reason: ContainersNotReady
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-11-10T18:08:23Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - image: rancher/local-path-provisioner:v0.0.21
      imageID: ""
      lastState: {}
      name: local-path-provisioner
      ready: false
      restartCount: 0
      started: false
      state:
        waiting:
          reason: ContainerCreating
    hostIP: 192.168.5.15
    phase: Pending
    qosClass: BestEffort
    startTime: "2022-11-10T18:08:23Z"
- metadata:
    creationTimestamp: "2022-11-10T18:02:20Z"
    generateName: metrics-server-5c8978b444-
    labels:
      k8s-app: metrics-server
      pod-template-hash: 5c8978b444
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:generateName: {}
          f:labels:
            .: {}
            f:k8s-app: {}
            f:pod-template-hash: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"b790a430-dd4a-4f39-a120-6685aebb49f6"}: {}
        f:spec:
          f:containers:
            k:{"name":"metrics-server"}:
              .: {}
              f:args: {}
              f:image: {}
              f:imagePullPolicy: {}
              f:livenessProbe:
                .: {}
                f:failureThreshold: {}
                f:httpGet:
                  .: {}
                  f:path: {}
                  f:port: {}
                  f:scheme: {}
                f:initialDelaySeconds: {}
                f:periodSeconds: {}
                f:successThreshold: {}
                f:timeoutSeconds: {}
              f:name: {}
              f:ports:
                .: {}
                k:{"containerPort":10250,"protocol":"TCP"}:
                  .: {}
                  f:containerPort: {}
                  f:name: {}
                  f:protocol: {}
              f:readinessProbe:
                .: {}
                f:failureThreshold: {}
                f:httpGet:
                  .: {}
                  f:path: {}
                  f:port: {}
                  f:scheme: {}
                f:periodSeconds: {}
                f:successThreshold: {}
                f:timeoutSeconds: {}
              f:resources:
                .: {}
                f:requests:
                  .: {}
                  f:cpu: {}
                  f:memory: {}
              f:securityContext:
                .: {}
                f:allowPrivilegeEscalation: {}
                f:readOnlyRootFilesystem: {}
                f:runAsNonRoot: {}
                f:runAsUser: {}
              f:terminationMessagePath: {}
              f:terminationMessagePolicy: {}
              f:volumeMounts:
                .: {}
                k:{"mountPath":"/tmp"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
          f:dnsPolicy: {}
          f:enableServiceLinks: {}
          f:priorityClassName: {}
          f:restartPolicy: {}
          f:schedulerName: {}
          f:securityContext: {}
          f:serviceAccount: {}
          f:serviceAccountName: {}
          f:terminationGracePeriodSeconds: {}
          f:tolerations: {}
          f:volumes:
            .: {}
            k:{"name":"tmp-dir"}:
              .: {}
              f:emptyDir: {}
              f:name: {}
      manager: k3s
      operation: Update
      time: "2022-11-10T18:02:20Z"
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          f:conditions:
            .: {}
            k:{"type":"ContainersReady"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:message: {}
              f:reason: {}
              f:status: {}
              f:type: {}
            k:{"type":"Initialized"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
            k:{"type":"PodScheduled"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:message: {}
              f:reason: {}
              f:status: {}
              f:type: {}
            k:{"type":"Ready"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:message: {}
              f:reason: {}
              f:status: {}
              f:type: {}
          f:containerStatuses: {}
          f:hostIP: {}
          f:startTime: {}
      manager: k3s
      operation: Update
      subresource: status
      time: "2022-11-10T18:08:23Z"
    name: metrics-server-5c8978b444-6lc2q
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: metrics-server-5c8978b444
      uid: b790a430-dd4a-4f39-a120-6685aebb49f6
    resourceVersion: "695"
    uid: 22f22bdc-82ec-4a94-98ef-d4897f69205d
  spec:
    containers:
    - args:
      - --cert-dir=/tmp
      - --secure-port=10250
      - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      - --kubelet-use-node-status-port
      - --metric-resolution=15s
      image: rancher/mirrored-metrics-server:v0.6.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /livez
          port: https
          scheme: HTTPS
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: metrics-server
      ports:
      - containerPort: 10250
        name: https
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /readyz
          port: https
          scheme: HTTPS
        periodSeconds: 2
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        requests:
          cpu: 100m
          memory: 70Mi
      securityContext:
        allowPrivilegeEscalation: false
        readOnlyRootFilesystem: true
        runAsNonRoot: true
        runAsUser: 1000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: tmp-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mtbng
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: lima-k3s-1
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: metrics-server
    serviceAccountName: metrics-server
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: tmp-dir
    - name: kube-api-access-mtbng
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-11-10T18:08:23Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-11-10T18:08:23Z"
      message: 'containers with unready status: [metrics-server]'
      reason: ContainersNotReady
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-11-10T18:08:23Z"
      message: 'containers with unready status: [metrics-server]'
      reason: ContainersNotReady
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-11-10T18:08:23Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - image: rancher/mirrored-metrics-server:v0.6.1
      imageID: ""
      lastState: {}
      name: metrics-server
      ready: false
      restartCount: 0
      started: false
      state:
        waiting:
          reason: ContainerCreating
    hostIP: 192.168.5.15
    phase: Pending
    qosClass: Burstable
    startTime: "2022-11-10T18:08:23Z"
- metadata:
    annotations:
      helmcharts.helm.cattle.io/configHash: SHA256=F9EDC16B1D75D180D7E3A6FDB18535233F1C2E6FECD623C0320EC068BBFEC077
    creationTimestamp: "2022-11-10T18:02:19Z"
    finalizers:
    - batch.kubernetes.io/job-tracking
    generateName: helm-install-traefik-
    labels:
      controller-uid: 5f568ea0-4dfc-474f-8b26-caf4684101ca
      helmcharts.helm.cattle.io/chart: traefik
      job-name: helm-install-traefik
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:helmcharts.helm.cattle.io/configHash: {}
          f:finalizers:
            .: {}
            v:"batch.kubernetes.io/job-tracking": {}
          f:generateName: {}
          f:labels:
            .: {}
            f:controller-uid: {}
            f:helmcharts.helm.cattle.io/chart: {}
            f:job-name: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5f568ea0-4dfc-474f-8b26-caf4684101ca"}: {}
        f:spec:
          f:containers:
            k:{"name":"helm"}:
              .: {}
              f:args: {}
              f:env:
                .: {}
                k:{"name":"CHART"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"CHART_NAMESPACE"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"FAILURE_POLICY"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"HELM_DRIVER"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"HELM_VERSION"}:
                  .: {}
                  f:name: {}
                k:{"name":"NAME"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"NO_PROXY"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"REPO"}:
                  .: {}
                  f:name: {}
                k:{"name":"TARGET_NAMESPACE"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"VERSION"}:
                  .: {}
                  f:name: {}
              f:image: {}
              f:imagePullPolicy: {}
              f:name: {}
              f:resources: {}
              f:terminationMessagePath: {}
              f:terminationMessagePolicy: {}
              f:volumeMounts:
                .: {}
                k:{"mountPath":"/chart"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/config"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
          f:dnsPolicy: {}
          f:enableServiceLinks: {}
          f:nodeSelector: {}
          f:restartPolicy: {}
          f:schedulerName: {}
          f:securityContext: {}
          f:serviceAccount: {}
          f:serviceAccountName: {}
          f:terminationGracePeriodSeconds: {}
          f:volumes:
            .: {}
            k:{"name":"content"}:
              .: {}
              f:configMap:
                .: {}
                f:defaultMode: {}
                f:name: {}
              f:name: {}
            k:{"name":"values"}:
              .: {}
              f:configMap:
                .: {}
                f:defaultMode: {}
                f:name: {}
              f:name: {}
      manager: k3s
      operation: Update
      time: "2022-11-10T18:02:19Z"
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          f:conditions:
            .: {}
            k:{"type":"ContainersReady"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:message: {}
              f:reason: {}
              f:status: {}
              f:type: {}
            k:{"type":"Initialized"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
            k:{"type":"PodScheduled"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:message: {}
              f:reason: {}
              f:status: {}
              f:type: {}
            k:{"type":"Ready"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:message: {}
              f:reason: {}
              f:status: {}
              f:type: {}
          f:containerStatuses: {}
          f:hostIP: {}
          f:startTime: {}
      manager: k3s
      operation: Update
      subresource: status
      time: "2022-11-10T18:08:25Z"
    name: helm-install-traefik-cdw9j
    namespace: kube-system
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: Job
      name: helm-install-traefik
      uid: 5f568ea0-4dfc-474f-8b26-caf4684101ca
    resourceVersion: "697"
    uid: dc9d323a-88a2-477c-b543-49f83a1bdb5a
  spec:
    containers:
    - args:
      - install
      - --set-string
      - global.systemDefaultRegistry=
      env:
      - name: NAME
        value: traefik
      - name: VERSION
      - name: REPO
      - name: HELM_DRIVER
        value: secret
      - name: CHART_NAMESPACE
        value: kube-system
      - name: CHART
        value: https://%{KUBERNETES_API}%/static/charts/traefik-12.0.000.tgz
      - name: HELM_VERSION
      - name: TARGET_NAMESPACE
        value: kube-system
      - name: NO_PROXY
        value: .svc,.cluster.local,10.42.0.0/16,10.43.0.0/16
      - name: FAILURE_POLICY
        value: reinstall
      image: rancher/klipper-helm:v0.7.3-build20220613
      imagePullPolicy: IfNotPresent
      name: helm
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /config
        name: values
      - mountPath: /chart
        name: content
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-t2kzj
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: lima-k3s-1
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: OnFailure
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: helm-traefik
    serviceAccountName: helm-traefik
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: chart-values-traefik
      name: values
    - configMap:
        defaultMode: 420
        name: chart-content-traefik
      name: content
    - name: kube-api-access-t2kzj
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-11-10T18:08:23Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-11-10T18:08:23Z"
      message: 'containers with unready status: [helm]'
      reason: ContainersNotReady
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-11-10T18:08:23Z"
      message: 'containers with unready status: [helm]'
      reason: ContainersNotReady
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-11-10T18:08:23Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - image: rancher/klipper-helm:v0.7.3-build20220613
      imageID: ""
      lastState: {}
      name: helm
      ready: false
      restartCount: 0
      started: false
      state:
        waiting:
          reason: ContainerCreating
    hostIP: 192.168.5.15
    phase: Pending
    qosClass: BestEffort
    startTime: "2022-11-10T18:08:23Z"
- metadata:
    annotations:
      helmcharts.helm.cattle.io/configHash: SHA256=E3B0C44298FC1C149AFBF4C8996FB92427AE41E4649B934CA495991B7852B855
    creationTimestamp: "2022-11-10T18:02:19Z"
    finalizers:
    - batch.kubernetes.io/job-tracking
    generateName: helm-install-traefik-crd-
    labels:
      controller-uid: fc83adf7-5e39-4087-b983-e86d2c3cc8a1
      helmcharts.helm.cattle.io/chart: traefik-crd
      job-name: helm-install-traefik-crd
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:helmcharts.helm.cattle.io/configHash: {}
          f:finalizers:
            .: {}
            v:"batch.kubernetes.io/job-tracking": {}
          f:generateName: {}
          f:labels:
            .: {}
            f:controller-uid: {}
            f:helmcharts.helm.cattle.io/chart: {}
            f:job-name: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fc83adf7-5e39-4087-b983-e86d2c3cc8a1"}: {}
        f:spec:
          f:containers:
            k:{"name":"helm"}:
              .: {}
              f:args: {}
              f:env:
                .: {}
                k:{"name":"CHART"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"CHART_NAMESPACE"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"FAILURE_POLICY"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"HELM_DRIVER"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"HELM_VERSION"}:
                  .: {}
                  f:name: {}
                k:{"name":"NAME"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"NO_PROXY"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"REPO"}:
                  .: {}
                  f:name: {}
                k:{"name":"TARGET_NAMESPACE"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"VERSION"}:
                  .: {}
                  f:name: {}
              f:image: {}
              f:imagePullPolicy: {}
              f:name: {}
              f:resources: {}
              f:terminationMessagePath: {}
              f:terminationMessagePolicy: {}
              f:volumeMounts:
                .: {}
                k:{"mountPath":"/chart"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/config"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
          f:dnsPolicy: {}
          f:enableServiceLinks: {}
          f:nodeSelector: {}
          f:restartPolicy: {}
          f:schedulerName: {}
          f:securityContext: {}
          f:serviceAccount: {}
          f:serviceAccountName: {}
          f:terminationGracePeriodSeconds: {}
          f:volumes:
            .: {}
            k:{"name":"content"}:
              .: {}
              f:configMap:
                .: {}
                f:defaultMode: {}
                f:name: {}
              f:name: {}
            k:{"name":"values"}:
              .: {}
              f:configMap:
                .: {}
                f:defaultMode: {}
                f:name: {}
              f:name: {}
      manager: k3s
      operation: Update
      time: "2022-11-10T18:02:19Z"
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          f:conditions:
            .: {}
            k:{"type":"ContainersReady"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:message: {}
              f:reason: {}
              f:status: {}
              f:type: {}
            k:{"type":"Initialized"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
            k:{"type":"PodScheduled"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:message: {}
              f:reason: {}
              f:status: {}
              f:type: {}
            k:{"type":"Ready"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:message: {}
              f:reason: {}
              f:status: {}
              f:type: {}
          f:containerStatuses: {}
          f:hostIP: {}
          f:startTime: {}
      manager: k3s
      operation: Update
      subresource: status
      time: "2022-11-10T18:08:25Z"
    name: helm-install-traefik-crd-8w2hr
    namespace: kube-system
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: Job
      name: helm-install-traefik-crd
      uid: fc83adf7-5e39-4087-b983-e86d2c3cc8a1
    resourceVersion: "698"
    uid: 3b98f5c4-2bca-4143-8d52-35a9e6df7ae2
  spec:
    containers:
    - args:
      - install
      env:
      - name: NAME
        value: traefik-crd
      - name: VERSION
      - name: REPO
      - name: HELM_DRIVER
        value: secret
      - name: CHART_NAMESPACE
        value: kube-system
      - name: CHART
        value: https://%{KUBERNETES_API}%/static/charts/traefik-crd-12.0.000.tgz
      - name: HELM_VERSION
      - name: TARGET_NAMESPACE
        value: kube-system
      - name: NO_PROXY
        value: .svc,.cluster.local,10.42.0.0/16,10.43.0.0/16
      - name: FAILURE_POLICY
        value: reinstall
      image: rancher/klipper-helm:v0.7.3-build20220613
      imagePullPolicy: IfNotPresent
      name: helm
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /config
        name: values
      - mountPath: /chart
        name: content
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-lr65q
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: lima-k3s-1
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: OnFailure
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: helm-traefik-crd
    serviceAccountName: helm-traefik-crd
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: chart-values-traefik-crd
      name: values
    - configMap:
        defaultMode: 420
        name: chart-content-traefik-crd
      name: content
    - name: kube-api-access-lr65q
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-11-10T18:08:23Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-11-10T18:08:23Z"
      message: 'containers with unready status: [helm]'
      reason: ContainersNotReady
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-11-10T18:08:23Z"
      message: 'containers with unready status: [helm]'
      reason: ContainersNotReady
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-11-10T18:08:23Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - image: rancher/klipper-helm:v0.7.3-build20220613
      imageID: ""
      lastState: {}
      name: helm
      ready: false
      restartCount: 0
      started: false
      state:
        waiting:
          reason: ContainerCreating
    hostIP: 192.168.5.15
    phase: Pending
    qosClass: BestEffort
    startTime: "2022-11-10T18:08:23Z"
- metadata:
    creationTimestamp: "2022-11-10T18:02:20Z"
    generateName: coredns-75fc8f8fff-
    labels:
      k8s-app: kube-dns
      pod-template-hash: 75fc8f8fff
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:generateName: {}
          f:labels:
            .: {}
            f:k8s-app: {}
            f:pod-template-hash: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"55f59c21-2219-4dc9-83f8-d3f9b9d5d56a"}: {}
        f:spec:
          f:containers:
            k:{"name":"coredns"}:
              .: {}
              f:args: {}
              f:image: {}
              f:imagePullPolicy: {}
              f:livenessProbe:
                .: {}
                f:failureThreshold: {}
                f:httpGet:
                  .: {}
                  f:path: {}
                  f:port: {}
                  f:scheme: {}
                f:initialDelaySeconds: {}
                f:periodSeconds: {}
                f:successThreshold: {}
                f:timeoutSeconds: {}
              f:name: {}
              f:ports:
                .: {}
                k:{"containerPort":53,"protocol":"TCP"}:
                  .: {}
                  f:containerPort: {}
                  f:name: {}
                  f:protocol: {}
                k:{"containerPort":53,"protocol":"UDP"}:
                  .: {}
                  f:containerPort: {}
                  f:name: {}
                  f:protocol: {}
                k:{"containerPort":9153,"protocol":"TCP"}:
                  .: {}
                  f:containerPort: {}
                  f:name: {}
                  f:protocol: {}
              f:readinessProbe:
                .: {}
                f:failureThreshold: {}
                f:httpGet:
                  .: {}
                  f:path: {}
                  f:port: {}
                  f:scheme: {}
                f:periodSeconds: {}
                f:successThreshold: {}
                f:timeoutSeconds: {}
              f:resources:
                .: {}
                f:limits:
                  .: {}
                  f:memory: {}
                f:requests:
                  .: {}
                  f:cpu: {}
                  f:memory: {}
              f:securityContext:
                .: {}
                f:allowPrivilegeEscalation: {}
                f:capabilities:
                  .: {}
                  f:add: {}
                  f:drop: {}
                f:readOnlyRootFilesystem: {}
              f:terminationMessagePath: {}
              f:terminationMessagePolicy: {}
              f:volumeMounts:
                .: {}
                k:{"mountPath":"/etc/coredns"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
                k:{"mountPath":"/etc/coredns/custom"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
          f:dnsPolicy: {}
          f:enableServiceLinks: {}
          f:nodeSelector: {}
          f:priorityClassName: {}
          f:restartPolicy: {}
          f:schedulerName: {}
          f:securityContext: {}
          f:serviceAccount: {}
          f:serviceAccountName: {}
          f:terminationGracePeriodSeconds: {}
          f:tolerations: {}
          f:topologySpreadConstraints:
            .: {}
            k:{"topologyKey":"kubernetes.io/hostname","whenUnsatisfiable":"DoNotSchedule"}:
              .: {}
              f:labelSelector: {}
              f:maxSkew: {}
              f:topologyKey: {}
              f:whenUnsatisfiable: {}
          f:volumes:
            .: {}
            k:{"name":"config-volume"}:
              .: {}
              f:configMap:
                .: {}
                f:defaultMode: {}
                f:items: {}
                f:name: {}
              f:name: {}
            k:{"name":"custom-config-volume"}:
              .: {}
              f:configMap:
                .: {}
                f:defaultMode: {}
                f:name: {}
                f:optional: {}
              f:name: {}
      manager: k3s
      operation: Update
      time: "2022-11-10T18:02:20Z"
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          f:conditions:
            .: {}
            k:{"type":"ContainersReady"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:message: {}
              f:reason: {}
              f:status: {}
              f:type: {}
            k:{"type":"Initialized"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
            k:{"type":"PodScheduled"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:message: {}
              f:reason: {}
              f:status: {}
              f:type: {}
            k:{"type":"Ready"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:message: {}
              f:reason: {}
              f:status: {}
              f:type: {}
          f:containerStatuses: {}
          f:hostIP: {}
          f:startTime: {}
      manager: k3s
      operation: Update
      subresource: status
      time: "2022-11-10T18:08:25Z"
    name: coredns-75fc8f8fff-qbkdn
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: coredns-75fc8f8fff
      uid: 55f59c21-2219-4dc9-83f8-d3f9b9d5d56a
    resourceVersion: "700"
    uid: 32e94538-fd14-4495-9c17-cd15e4171f89
  spec:
    containers:
    - args:
      - -conf
      - /etc/coredns/Corefile
      image: rancher/mirrored-coredns-coredns:1.9.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: coredns
      ports:
      - containerPort: 53
        name: dns
        protocol: UDP
      - containerPort: 53
        name: dns-tcp
        protocol: TCP
      - containerPort: 9153
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: 8181
          scheme: HTTP
        periodSeconds: 2
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          memory: 170Mi
        requests:
          cpu: 100m
          memory: 70Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          add:
          - NET_BIND_SERVICE
          drop:
          - all
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/coredns
        name: config-volume
        readOnly: true
      - mountPath: /etc/coredns/custom
        name: custom-config-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-42965
        readOnly: true
    dnsPolicy: Default
    enableServiceLinks: true
    nodeName: lima-k3s-1
    nodeSelector:
      beta.kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: coredns
    serviceAccountName: coredns
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    topologySpreadConstraints:
    - labelSelector:
        matchLabels:
          k8s-app: kube-dns
      maxSkew: 1
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: DoNotSchedule
    volumes:
    - configMap:
        defaultMode: 420
        items:
        - key: Corefile
          path: Corefile
        - key: NodeHosts
          path: NodeHosts
        name: coredns
      name: config-volume
    - configMap:
        defaultMode: 420
        name: coredns-custom
        optional: true
      name: custom-config-volume
    - name: kube-api-access-42965
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-11-10T18:08:23Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-11-10T18:08:23Z"
      message: 'containers with unready status: [coredns]'
      reason: ContainersNotReady
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-11-10T18:08:23Z"
      message: 'containers with unready status: [coredns]'
      reason: ContainersNotReady
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-11-10T18:08:23Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - image: rancher/mirrored-coredns-coredns:1.9.1
      imageID: ""
      lastState: {}
      name: coredns
      ready: false
      restartCount: 0
      started: false
      state:
        waiting:
          reason: ContainerCreating
    hostIP: 192.168.5.15
    phase: Pending
    qosClass: Burstable
    startTime: "2022-11-10T18:08:23Z"
- metadata:
    creationTimestamp: "2022-11-10T18:50:27Z"
    generateName: cilium-operator-76c6d95785-
    labels:
      io.cilium/app: operator
      name: cilium-operator
      pod-template-hash: 76c6d95785
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:generateName: {}
          f:labels:
            .: {}
            f:io.cilium/app: {}
            f:name: {}
            f:pod-template-hash: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e97786d4-2f69-496a-a93e-4dc5f3f353f0"}: {}
        f:spec:
          f:affinity:
            .: {}
            f:podAntiAffinity:
              .: {}
              f:requiredDuringSchedulingIgnoredDuringExecution: {}
          f:containers:
            k:{"name":"cilium-operator"}:
              .: {}
              f:args: {}
              f:command: {}
              f:env:
                .: {}
                k:{"name":"CILIUM_DEBUG"}:
                  .: {}
                  f:name: {}
                  f:valueFrom:
                    .: {}
                    f:configMapKeyRef: {}
                k:{"name":"CILIUM_K8S_NAMESPACE"}:
                  .: {}
                  f:name: {}
                  f:valueFrom:
                    .: {}
                    f:fieldRef: {}
                k:{"name":"K8S_NODE_NAME"}:
                  .: {}
                  f:name: {}
                  f:valueFrom:
                    .: {}
                    f:fieldRef: {}
              f:image: {}
              f:imagePullPolicy: {}
              f:livenessProbe:
                .: {}
                f:failureThreshold: {}
                f:httpGet:
                  .: {}
                  f:host: {}
                  f:path: {}
                  f:port: {}
                  f:scheme: {}
                f:initialDelaySeconds: {}
                f:periodSeconds: {}
                f:successThreshold: {}
                f:timeoutSeconds: {}
              f:name: {}
              f:resources: {}
              f:terminationMessagePath: {}
              f:terminationMessagePolicy: {}
              f:volumeMounts:
                .: {}
                k:{"mountPath":"/tmp/cilium/config-map"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
          f:dnsPolicy: {}
          f:enableServiceLinks: {}
          f:hostNetwork: {}
          f:nodeSelector: {}
          f:priorityClassName: {}
          f:restartPolicy: {}
          f:schedulerName: {}
          f:securityContext: {}
          f:serviceAccount: {}
          f:serviceAccountName: {}
          f:terminationGracePeriodSeconds: {}
          f:tolerations: {}
          f:volumes:
            .: {}
            k:{"name":"cilium-config-path"}:
              .: {}
              f:configMap:
                .: {}
                f:defaultMode: {}
                f:name: {}
              f:name: {}
      manager: k3s
      operation: Update
      time: "2022-11-10T18:50:27Z"
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          f:conditions:
            .: {}
            k:{"type":"PodScheduled"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:message: {}
              f:reason: {}
              f:status: {}
              f:type: {}
      manager: k3s
      operation: Update
      subresource: status
      time: "2022-11-10T18:50:27Z"
    name: cilium-operator-76c6d95785-tzv8t
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: cilium-operator-76c6d95785
      uid: e97786d4-2f69-496a-a93e-4dc5f3f353f0
    resourceVersion: "1648"
    uid: 264ad706-0d97-44db-ad42-177d928b7eba
  spec:
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              io.cilium/app: operator
          topologyKey: kubernetes.io/hostname
    containers:
    - args:
      - --config-dir=/tmp/cilium/config-map
      - --debug=$(CILIUM_DEBUG)
      command:
      - cilium-operator-generic
      env:
      - name: K8S_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: CILIUM_K8S_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CILIUM_DEBUG
        valueFrom:
          configMapKeyRef:
            key: debug
            name: cilium-config
            optional: true
      image: quay.io/cilium/operator-generic:v1.12.3@sha256:816ec1da586139b595eeb31932c61a7c13b07fb4a0255341c0e0f18608e84eff
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          host: 127.0.0.1
          path: /healthz
          port: 9234
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 3
      name: cilium-operator
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /tmp/cilium/config-map
        name: cilium-config-path
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-4vzxs
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: cilium-operator
    serviceAccountName: cilium-operator
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    volumes:
    - configMap:
        defaultMode: 420
        name: cilium-config
      name: cilium-config-path
    - name: kube-api-access-4vzxs
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-11-10T18:50:27Z"
      message: '0/1 nodes are available: 1 node(s) didn''t match pod anti-affinity
        rules. preemption: 0/1 nodes are available: 1 node(s) didn''t match pod anti-affinity
        rules.'
      reason: Unschedulable
      status: "False"
      type: PodScheduled
    phase: Pending
    qosClass: BestEffort
- metadata:
    creationTimestamp: "2022-11-10T18:50:27Z"
    generateName: cilium-operator-76c6d95785-
    labels:
      io.cilium/app: operator
      name: cilium-operator
      pod-template-hash: 76c6d95785
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:generateName: {}
          f:labels:
            .: {}
            f:io.cilium/app: {}
            f:name: {}
            f:pod-template-hash: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e97786d4-2f69-496a-a93e-4dc5f3f353f0"}: {}
        f:spec:
          f:affinity:
            .: {}
            f:podAntiAffinity:
              .: {}
              f:requiredDuringSchedulingIgnoredDuringExecution: {}
          f:containers:
            k:{"name":"cilium-operator"}:
              .: {}
              f:args: {}
              f:command: {}
              f:env:
                .: {}
                k:{"name":"CILIUM_DEBUG"}:
                  .: {}
                  f:name: {}
                  f:valueFrom:
                    .: {}
                    f:configMapKeyRef: {}
                k:{"name":"CILIUM_K8S_NAMESPACE"}:
                  .: {}
                  f:name: {}
                  f:valueFrom:
                    .: {}
                    f:fieldRef: {}
                k:{"name":"K8S_NODE_NAME"}:
                  .: {}
                  f:name: {}
                  f:valueFrom:
                    .: {}
                    f:fieldRef: {}
              f:image: {}
              f:imagePullPolicy: {}
              f:livenessProbe:
                .: {}
                f:failureThreshold: {}
                f:httpGet:
                  .: {}
                  f:host: {}
                  f:path: {}
                  f:port: {}
                  f:scheme: {}
                f:initialDelaySeconds: {}
                f:periodSeconds: {}
                f:successThreshold: {}
                f:timeoutSeconds: {}
              f:name: {}
              f:resources: {}
              f:terminationMessagePath: {}
              f:terminationMessagePolicy: {}
              f:volumeMounts:
                .: {}
                k:{"mountPath":"/tmp/cilium/config-map"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
          f:dnsPolicy: {}
          f:enableServiceLinks: {}
          f:hostNetwork: {}
          f:nodeSelector: {}
          f:priorityClassName: {}
          f:restartPolicy: {}
          f:schedulerName: {}
          f:securityContext: {}
          f:serviceAccount: {}
          f:serviceAccountName: {}
          f:terminationGracePeriodSeconds: {}
          f:tolerations: {}
          f:volumes:
            .: {}
            k:{"name":"cilium-config-path"}:
              .: {}
              f:configMap:
                .: {}
                f:defaultMode: {}
                f:name: {}
              f:name: {}
      manager: k3s
      operation: Update
      time: "2022-11-10T18:50:27Z"
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          f:conditions:
            k:{"type":"ContainersReady"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:message: {}
              f:reason: {}
              f:status: {}
              f:type: {}
            k:{"type":"Initialized"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
            k:{"type":"Ready"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:message: {}
              f:reason: {}
              f:status: {}
              f:type: {}
          f:containerStatuses: {}
          f:hostIP: {}
          f:phase: {}
          f:podIP: {}
          f:podIPs:
            .: {}
            k:{"ip":"192.168.5.15"}:
              .: {}
              f:ip: {}
            k:{"ip":"fec0::5055:55ff:fed4:b233"}:
              .: {}
              f:ip: {}
          f:startTime: {}
      manager: k3s
      operation: Update
      subresource: status
      time: "2022-11-10T18:57:37Z"
    name: cilium-operator-76c6d95785-jzz9l
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: cilium-operator-76c6d95785
      uid: e97786d4-2f69-496a-a93e-4dc5f3f353f0
    resourceVersion: "1856"
    uid: a25bb8fd-1fcb-465f-81a9-6e998d2c4f21
  spec:
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              io.cilium/app: operator
          topologyKey: kubernetes.io/hostname
    containers:
    - args:
      - --config-dir=/tmp/cilium/config-map
      - --debug=$(CILIUM_DEBUG)
      command:
      - cilium-operator-generic
      env:
      - name: K8S_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: CILIUM_K8S_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CILIUM_DEBUG
        valueFrom:
          configMapKeyRef:
            key: debug
            name: cilium-config
            optional: true
      image: quay.io/cilium/operator-generic:v1.12.3@sha256:816ec1da586139b595eeb31932c61a7c13b07fb4a0255341c0e0f18608e84eff
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          host: 127.0.0.1
          path: /healthz
          port: 9234
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 3
      name: cilium-operator
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /tmp/cilium/config-map
        name: cilium-config-path
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-9cc5s
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: lima-k3s-1
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: cilium-operator
    serviceAccountName: cilium-operator
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    volumes:
    - configMap:
        defaultMode: 420
        name: cilium-config
      name: cilium-config-path
    - name: kube-api-access-9cc5s
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-11-10T18:50:27Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-11-10T18:57:36Z"
      message: 'containers with unready status: [cilium-operator]'
      reason: ContainersNotReady
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-11-10T18:57:36Z"
      message: 'containers with unready status: [cilium-operator]'
      reason: ContainersNotReady
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-11-10T18:50:27Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://3300529be851ef35f533d145459c42c37edd78a1ce4774cd5db6ae902ed3c40a
      image: sha256:4fe334b260dfcd25fc037f73c71d14f69bf3d992f7e5a503ea6fa37cb3db24b1
      imageID: quay.io/cilium/operator-generic@sha256:816ec1da586139b595eeb31932c61a7c13b07fb4a0255341c0e0f18608e84eff
      lastState:
        terminated:
          containerID: containerd://3300529be851ef35f533d145459c42c37edd78a1ce4774cd5db6ae902ed3c40a
          exitCode: 1
          finishedAt: "2022-11-10T18:57:35Z"
          message: |
            sg="  --nodes-gc-interval='5m0s'" subsys=cilium-operator-generic
            level=info msg="  --operator-api-serve-addr='127.0.0.1:9234'" subsys=cilium-operator-generic
            level=info msg="  --operator-prometheus-serve-addr=':9963'" subsys=cilium-operator-generic
            level=info msg="  --parallel-alloc-workers='50'" subsys=cilium-operator-generic
            level=info msg="  --pprof='false'" subsys=cilium-operator-generic
            level=info msg="  --pprof-port='6061'" subsys=cilium-operator-generic
            level=info msg="  --remove-cilium-node-taints='true'" subsys=cilium-operator-generic
            level=info msg="  --set-cilium-is-up-condition='true'" subsys=cilium-operator-generic
            level=info msg="  --skip-crd-creation='false'" subsys=cilium-operator-generic
            level=info msg="  --subnet-ids-filter=''" subsys=cilium-operator-generic
            level=info msg="  --subnet-tags-filter=''" subsys=cilium-operator-generic
            level=info msg="  --synchronize-k8s-nodes='true'" subsys=cilium-operator-generic
            level=info msg="  --synchronize-k8s-services='true'" subsys=cilium-operator-generic
            level=info msg="  --unmanaged-pod-watcher-interval='15'" subsys=cilium-operator-generic
            level=info msg="  --version='false'" subsys=cilium-operator-generic
            level=info msg="Cilium Operator 1.12.3 1c466d2 2022-10-12T11:33:37+01:00 go version go1.18.6 linux/arm64" subsys=cilium-operator-generic
            level=info msg="Starting apiserver on address 127.0.0.1:9234" subsys=cilium-operator-api
            level=info msg="Establishing connection to apiserver" host="https://10.43.0.1:443" subsys=k8s
            level=info msg="Establishing connection to apiserver" host="https://10.43.0.1:443" subsys=k8s
            level=error msg="Unable to contact k8s api-server" error="Get \"https://10.43.0.1:443/api/v1/namespaces/kube-system\": dial tcp 10.43.0.1:443: i/o timeout" ipAddr="https://10.43.0.1:443" subsys=k8s
            level=fatal msg="Unable to connect to Kubernetes apiserver" error="unable to create k8s client: unable to create k8s client: Get \"https://10.43.0.1:443/api/v1/namespaces/kube-system\": dial tcp 10.43.0.1:443: i/o timeout" subsys=cilium-operator-generic
          reason: Error
          startedAt: "2022-11-10T18:56:30Z"
      name: cilium-operator
      ready: false
      restartCount: 4
      started: false
      state:
        waiting:
          message: back-off 1m20s restarting failed container=cilium-operator pod=cilium-operator-76c6d95785-jzz9l_kube-system(a25bb8fd-1fcb-465f-81a9-6e998d2c4f21)
          reason: CrashLoopBackOff
    hostIP: 192.168.5.15
    phase: Running
    podIP: 192.168.5.15
    podIPs:
    - ip: 192.168.5.15
    - ip: fec0::5055:55ff:fed4:b233
    qosClass: BestEffort
    startTime: "2022-11-10T18:50:27Z"
- metadata:
    annotations:
      container.apparmor.security.beta.kubernetes.io/apply-sysctl-overwrites: unconfined
      container.apparmor.security.beta.kubernetes.io/cilium-agent: unconfined
      container.apparmor.security.beta.kubernetes.io/clean-cilium-state: unconfined
      container.apparmor.security.beta.kubernetes.io/mount-cgroup: unconfined
    creationTimestamp: "2022-11-10T18:50:27Z"
    generateName: cilium-
    labels:
      controller-revision-hash: c478654d
      k8s-app: cilium
      pod-template-generation: "1"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:container.apparmor.security.beta.kubernetes.io/apply-sysctl-overwrites: {}
            f:container.apparmor.security.beta.kubernetes.io/cilium-agent: {}
            f:container.apparmor.security.beta.kubernetes.io/clean-cilium-state: {}
            f:container.apparmor.security.beta.kubernetes.io/mount-cgroup: {}
          f:generateName: {}
          f:labels:
            .: {}
            f:controller-revision-hash: {}
            f:k8s-app: {}
            f:pod-template-generation: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"4cc4e884-daa4-4bc3-a49d-baf3a7aed8ce"}: {}
        f:spec:
          f:affinity:
            .: {}
            f:nodeAffinity:
              .: {}
              f:requiredDuringSchedulingIgnoredDuringExecution: {}
            f:podAntiAffinity:
              .: {}
              f:requiredDuringSchedulingIgnoredDuringExecution: {}
          f:containers:
            k:{"name":"cilium-agent"}:
              .: {}
              f:args: {}
              f:command: {}
              f:env:
                .: {}
                k:{"name":"CILIUM_CLUSTERMESH_CONFIG"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"CILIUM_CNI_CHAINING_MODE"}:
                  .: {}
                  f:name: {}
                  f:valueFrom:
                    .: {}
                    f:configMapKeyRef: {}
                k:{"name":"CILIUM_CUSTOM_CNI_CONF"}:
                  .: {}
                  f:name: {}
                  f:valueFrom:
                    .: {}
                    f:configMapKeyRef: {}
                k:{"name":"CILIUM_K8S_NAMESPACE"}:
                  .: {}
                  f:name: {}
                  f:valueFrom:
                    .: {}
                    f:fieldRef: {}
                k:{"name":"K8S_NODE_NAME"}:
                  .: {}
                  f:name: {}
                  f:valueFrom:
                    .: {}
                    f:fieldRef: {}
              f:image: {}
              f:imagePullPolicy: {}
              f:lifecycle:
                .: {}
                f:postStart:
                  .: {}
                  f:exec:
                    .: {}
                    f:command: {}
                f:preStop:
                  .: {}
                  f:exec:
                    .: {}
                    f:command: {}
              f:livenessProbe:
                .: {}
                f:failureThreshold: {}
                f:httpGet:
                  .: {}
                  f:host: {}
                  f:httpHeaders: {}
                  f:path: {}
                  f:port: {}
                  f:scheme: {}
                f:periodSeconds: {}
                f:successThreshold: {}
                f:timeoutSeconds: {}
              f:name: {}
              f:readinessProbe:
                .: {}
                f:failureThreshold: {}
                f:httpGet:
                  .: {}
                  f:host: {}
                  f:httpHeaders: {}
                  f:path: {}
                  f:port: {}
                  f:scheme: {}
                f:periodSeconds: {}
                f:successThreshold: {}
                f:timeoutSeconds: {}
              f:resources: {}
              f:securityContext:
                .: {}
                f:capabilities:
                  .: {}
                  f:add: {}
                  f:drop: {}
                f:seLinuxOptions:
                  .: {}
                  f:level: {}
                  f:type: {}
              f:startupProbe:
                .: {}
                f:failureThreshold: {}
                f:httpGet:
                  .: {}
                  f:host: {}
                  f:httpHeaders: {}
                  f:path: {}
                  f:port: {}
                  f:scheme: {}
                f:periodSeconds: {}
                f:successThreshold: {}
                f:timeoutSeconds: {}
              f:terminationMessagePath: {}
              f:terminationMessagePolicy: {}
              f:volumeMounts:
                .: {}
                k:{"mountPath":"/host/etc/cni/net.d"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/host/opt/cni/bin"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/host/proc/sys/kernel"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/host/proc/sys/net"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/lib/modules"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
                k:{"mountPath":"/run/xtables.lock"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/sys/fs/bpf"}:
                  .: {}
                  f:mountPath: {}
                  f:mountPropagation: {}
                  f:name: {}
                k:{"mountPath":"/tmp/cilium/config-map"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
                k:{"mountPath":"/var/lib/cilium/clustermesh"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
                k:{"mountPath":"/var/lib/cilium/tls/hubble"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
                k:{"mountPath":"/var/run/cilium"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
          f:dnsPolicy: {}
          f:enableServiceLinks: {}
          f:hostNetwork: {}
          f:initContainers:
            .: {}
            k:{"name":"apply-sysctl-overwrites"}:
              .: {}
              f:command: {}
              f:env:
                .: {}
                k:{"name":"BIN_PATH"}:
                  .: {}
                  f:name: {}
                  f:value: {}
              f:image: {}
              f:imagePullPolicy: {}
              f:name: {}
              f:resources: {}
              f:securityContext:
                .: {}
                f:capabilities:
                  .: {}
                  f:add: {}
                  f:drop: {}
                f:seLinuxOptions:
                  .: {}
                  f:level: {}
                  f:type: {}
              f:terminationMessagePath: {}
              f:terminationMessagePolicy: {}
              f:volumeMounts:
                .: {}
                k:{"mountPath":"/hostbin"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/hostproc"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
            k:{"name":"clean-cilium-state"}:
              .: {}
              f:command: {}
              f:env:
                .: {}
                k:{"name":"CILIUM_ALL_STATE"}:
                  .: {}
                  f:name: {}
                  f:valueFrom:
                    .: {}
                    f:configMapKeyRef: {}
                k:{"name":"CILIUM_BPF_STATE"}:
                  .: {}
                  f:name: {}
                  f:valueFrom:
                    .: {}
                    f:configMapKeyRef: {}
              f:image: {}
              f:imagePullPolicy: {}
              f:name: {}
              f:resources:
                .: {}
                f:requests:
                  .: {}
                  f:cpu: {}
                  f:memory: {}
              f:securityContext:
                .: {}
                f:capabilities:
                  .: {}
                  f:add: {}
                  f:drop: {}
                f:seLinuxOptions:
                  .: {}
                  f:level: {}
                  f:type: {}
              f:terminationMessagePath: {}
              f:terminationMessagePolicy: {}
              f:volumeMounts:
                .: {}
                k:{"mountPath":"/run/cilium/cgroupv2"}:
                  .: {}
                  f:mountPath: {}
                  f:mountPropagation: {}
                  f:name: {}
                k:{"mountPath":"/sys/fs/bpf"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/var/run/cilium"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
            k:{"name":"mount-bpf-fs"}:
              .: {}
              f:args: {}
              f:command: {}
              f:image: {}
              f:imagePullPolicy: {}
              f:name: {}
              f:resources: {}
              f:securityContext:
                .: {}
                f:privileged: {}
              f:terminationMessagePath: {}
              f:terminationMessagePolicy: {}
              f:volumeMounts:
                .: {}
                k:{"mountPath":"/sys/fs/bpf"}:
                  .: {}
                  f:mountPath: {}
                  f:mountPropagation: {}
                  f:name: {}
            k:{"name":"mount-cgroup"}:
              .: {}
              f:command: {}
              f:env:
                .: {}
                k:{"name":"BIN_PATH"}:
                  .: {}
                  f:name: {}
                  f:value: {}
                k:{"name":"CGROUP_ROOT"}:
                  .: {}
                  f:name: {}
                  f:value: {}
              f:image: {}
              f:imagePullPolicy: {}
              f:name: {}
              f:resources: {}
              f:securityContext:
                .: {}
                f:capabilities:
                  .: {}
                  f:add: {}
                  f:drop: {}
                f:seLinuxOptions:
                  .: {}
                  f:level: {}
                  f:type: {}
              f:terminationMessagePath: {}
              f:terminationMessagePolicy: {}
              f:volumeMounts:
                .: {}
                k:{"mountPath":"/hostbin"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/hostproc"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
          f:nodeSelector: {}
          f:priorityClassName: {}
          f:restartPolicy: {}
          f:schedulerName: {}
          f:securityContext: {}
          f:serviceAccount: {}
          f:serviceAccountName: {}
          f:terminationGracePeriodSeconds: {}
          f:tolerations: {}
          f:volumes:
            .: {}
            k:{"name":"bpf-maps"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"cilium-cgroup"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"cilium-config-path"}:
              .: {}
              f:configMap:
                .: {}
                f:defaultMode: {}
                f:name: {}
              f:name: {}
            k:{"name":"cilium-run"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"clustermesh-secrets"}:
              .: {}
              f:name: {}
              f:secret:
                .: {}
                f:defaultMode: {}
                f:optional: {}
                f:secretName: {}
            k:{"name":"cni-path"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"etc-cni-netd"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"host-proc-sys-kernel"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"host-proc-sys-net"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"hostproc"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"hubble-tls"}:
              .: {}
              f:name: {}
              f:projected:
                .: {}
                f:defaultMode: {}
                f:sources: {}
            k:{"name":"lib-modules"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"xtables-lock"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
      manager: k3s
      operation: Update
      time: "2022-11-10T18:50:27Z"
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          f:conditions:
            k:{"type":"ContainersReady"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:message: {}
              f:reason: {}
              f:status: {}
              f:type: {}
            k:{"type":"Initialized"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
            k:{"type":"Ready"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:message: {}
              f:reason: {}
              f:status: {}
              f:type: {}
          f:containerStatuses: {}
          f:ephemeralContainerStatuses: {}
          f:hostIP: {}
          f:initContainerStatuses: {}
          f:phase: {}
          f:podIP: {}
          f:podIPs:
            .: {}
            k:{"ip":"192.168.5.15"}:
              .: {}
              f:ip: {}
            k:{"ip":"fec0::5055:55ff:fed4:b233"}:
              .: {}
              f:ip: {}
          f:startTime: {}
      manager: k3s
      operation: Update
      subresource: status
      time: "2022-11-10T18:58:35Z"
    name: cilium-cbzlp
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: cilium
      uid: 4cc4e884-daa4-4bc3-a49d-baf3a7aed8ce
    resourceVersion: "1883"
    uid: f431a8c6-1f57-4db6-9679-472213eda73c
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - lima-k3s-1
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              k8s-app: cilium
          topologyKey: kubernetes.io/hostname
    containers:
    - args:
      - --config-dir=/tmp/cilium/config-map
      command:
      - cilium-agent
      env:
      - name: K8S_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: CILIUM_K8S_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CILIUM_CLUSTERMESH_CONFIG
        value: /var/lib/cilium/clustermesh/
      - name: CILIUM_CNI_CHAINING_MODE
        valueFrom:
          configMapKeyRef:
            key: cni-chaining-mode
            name: cilium-config
            optional: true
      - name: CILIUM_CUSTOM_CNI_CONF
        valueFrom:
          configMapKeyRef:
            key: custom-cni-conf
            name: cilium-config
            optional: true
      image: quay.io/cilium/cilium:v1.12.3@sha256:30de50c4dc0a1e1077e9e7917a54d5cab253058b3f779822aec00f5c817ca826
      imagePullPolicy: IfNotPresent
      lifecycle:
        postStart:
          exec:
            command:
            - /cni-install.sh
            - --enable-debug=false
            - --cni-exclusive=true
            - --log-file=/var/run/cilium/cilium-cni.log
        preStop:
          exec:
            command:
            - /cni-uninstall.sh
      livenessProbe:
        failureThreshold: 10
        httpGet:
          host: 127.0.0.1
          httpHeaders:
          - name: brief
            value: "true"
          path: /healthz
          port: 9879
          scheme: HTTP
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 5
      name: cilium-agent
      readinessProbe:
        failureThreshold: 3
        httpGet:
          host: 127.0.0.1
          httpHeaders:
          - name: brief
            value: "true"
          path: /healthz
          port: 9879
          scheme: HTTP
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 5
      resources: {}
      securityContext:
        capabilities:
          add:
          - CHOWN
          - KILL
          - NET_ADMIN
          - NET_RAW
          - IPC_LOCK
          - SYS_MODULE
          - SYS_ADMIN
          - SYS_RESOURCE
          - DAC_OVERRIDE
          - FOWNER
          - SETGID
          - SETUID
          drop:
          - ALL
        seLinuxOptions:
          level: s0
          type: spc_t
      startupProbe:
        failureThreshold: 105
        httpGet:
          host: 127.0.0.1
          httpHeaders:
          - name: brief
            value: "true"
          path: /healthz
          port: 9879
          scheme: HTTP
        periodSeconds: 2
        successThreshold: 1
        timeoutSeconds: 1
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /host/proc/sys/net
        name: host-proc-sys-net
      - mountPath: /host/proc/sys/kernel
        name: host-proc-sys-kernel
      - mountPath: /sys/fs/bpf
        mountPropagation: HostToContainer
        name: bpf-maps
      - mountPath: /var/run/cilium
        name: cilium-run
      - mountPath: /host/opt/cni/bin
        name: cni-path
      - mountPath: /host/etc/cni/net.d
        name: etc-cni-netd
      - mountPath: /var/lib/cilium/clustermesh
        name: clustermesh-secrets
        readOnly: true
      - mountPath: /tmp/cilium/config-map
        name: cilium-config-path
        readOnly: true
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /var/lib/cilium/tls/hubble
        name: hubble-tls
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-f6bjb
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    ephemeralContainers:
    - command:
      - /bin/sleep
      - 1d
      env:
      - name: K8S_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: CILIUM_K8S_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CILIUM_CLUSTERMESH_CONFIG
        value: /var/lib/cilium/clustermesh/
      - name: CILIUM_CNI_CHAINING_MODE
        valueFrom:
          configMapKeyRef:
            key: cni-chaining-mode
            name: cilium-config
            optional: true
      - name: CILIUM_CUSTOM_CNI_CONF
        valueFrom:
          configMapKeyRef:
            key: custom-cni-conf
            name: cilium-config
            optional: true
      image: quay.io/cilium/cilium:v1.12.3@sha256:30de50c4dc0a1e1077e9e7917a54d5cab253058b3f779822aec00f5c817ca826
      imagePullPolicy: IfNotPresent
      name: sysdump-1668106631
      resources: {}
      securityContext:
        capabilities:
          add:
          - CHOWN
          - KILL
          - NET_ADMIN
          - NET_RAW
          - IPC_LOCK
          - SYS_MODULE
          - SYS_ADMIN
          - SYS_RESOURCE
          - DAC_OVERRIDE
          - FOWNER
          - SETGID
          - SETUID
          drop:
          - ALL
        seLinuxOptions:
          level: s0
          type: spc_t
      targetContainerName: cilium-agent
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/proc/sys/net
        name: host-proc-sys-net
      - mountPath: /host/proc/sys/kernel
        name: host-proc-sys-kernel
      - mountPath: /sys/fs/bpf
        mountPropagation: HostToContainer
        name: bpf-maps
      - mountPath: /var/run/cilium
        name: cilium-run
      - mountPath: /host/opt/cni/bin
        name: cni-path
      - mountPath: /host/etc/cni/net.d
        name: etc-cni-netd
      - mountPath: /var/lib/cilium/clustermesh
        name: clustermesh-secrets
        readOnly: true
      - mountPath: /tmp/cilium/config-map
        name: cilium-config-path
        readOnly: true
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /var/lib/cilium/tls/hubble
        name: hubble-tls
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-f6bjb
        readOnly: true
    hostNetwork: true
    initContainers:
    - command:
      - sh
      - -ec
      - |
        cp /usr/bin/cilium-mount /hostbin/cilium-mount;
        nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt "${BIN_PATH}/cilium-mount" $CGROUP_ROOT;
        rm /hostbin/cilium-mount
      env:
      - name: CGROUP_ROOT
        value: /run/cilium/cgroupv2
      - name: BIN_PATH
        value: /opt/cni/bin
      image: quay.io/cilium/cilium:v1.12.3@sha256:30de50c4dc0a1e1077e9e7917a54d5cab253058b3f779822aec00f5c817ca826
      imagePullPolicy: IfNotPresent
      name: mount-cgroup
      resources: {}
      securityContext:
        capabilities:
          add:
          - SYS_ADMIN
          - SYS_CHROOT
          - SYS_PTRACE
          drop:
          - ALL
        seLinuxOptions:
          level: s0
          type: spc_t
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /hostproc
        name: hostproc
      - mountPath: /hostbin
        name: cni-path
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-f6bjb
        readOnly: true
    - command:
      - sh
      - -ec
      - |
        cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;
        nsenter --mount=/hostproc/1/ns/mnt "${BIN_PATH}/cilium-sysctlfix";
        rm /hostbin/cilium-sysctlfix
      env:
      - name: BIN_PATH
        value: /opt/cni/bin
      image: quay.io/cilium/cilium:v1.12.3@sha256:30de50c4dc0a1e1077e9e7917a54d5cab253058b3f779822aec00f5c817ca826
      imagePullPolicy: IfNotPresent
      name: apply-sysctl-overwrites
      resources: {}
      securityContext:
        capabilities:
          add:
          - SYS_ADMIN
          - SYS_CHROOT
          - SYS_PTRACE
          drop:
          - ALL
        seLinuxOptions:
          level: s0
          type: spc_t
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /hostproc
        name: hostproc
      - mountPath: /hostbin
        name: cni-path
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-f6bjb
        readOnly: true
    - args:
      - mount | grep "/sys/fs/bpf type bpf" || mount -t bpf bpf /sys/fs/bpf
      command:
      - /bin/bash
      - -c
      - --
      image: quay.io/cilium/cilium:v1.12.3@sha256:30de50c4dc0a1e1077e9e7917a54d5cab253058b3f779822aec00f5c817ca826
      imagePullPolicy: IfNotPresent
      name: mount-bpf-fs
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /sys/fs/bpf
        mountPropagation: Bidirectional
        name: bpf-maps
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-f6bjb
        readOnly: true
    - command:
      - /init-container.sh
      env:
      - name: CILIUM_ALL_STATE
        valueFrom:
          configMapKeyRef:
            key: clean-cilium-state
            name: cilium-config
            optional: true
      - name: CILIUM_BPF_STATE
        valueFrom:
          configMapKeyRef:
            key: clean-cilium-bpf-state
            name: cilium-config
            optional: true
      image: quay.io/cilium/cilium:v1.12.3@sha256:30de50c4dc0a1e1077e9e7917a54d5cab253058b3f779822aec00f5c817ca826
      imagePullPolicy: IfNotPresent
      name: clean-cilium-state
      resources:
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
          - SYS_MODULE
          - SYS_ADMIN
          - SYS_RESOURCE
          drop:
          - ALL
        seLinuxOptions:
          level: s0
          type: spc_t
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /sys/fs/bpf
        name: bpf-maps
      - mountPath: /run/cilium/cgroupv2
        mountPropagation: HostToContainer
        name: cilium-cgroup
      - mountPath: /var/run/cilium
        name: cilium-run
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-f6bjb
        readOnly: true
    nodeName: lima-k3s-1
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: cilium
    serviceAccountName: cilium
    terminationGracePeriodSeconds: 1
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/run/cilium
        type: DirectoryOrCreate
      name: cilium-run
    - hostPath:
        path: /sys/fs/bpf
        type: DirectoryOrCreate
      name: bpf-maps
    - hostPath:
        path: /proc
        type: Directory
      name: hostproc
    - hostPath:
        path: /run/cilium/cgroupv2
        type: DirectoryOrCreate
      name: cilium-cgroup
    - hostPath:
        path: /opt/cni/bin
        type: DirectoryOrCreate
      name: cni-path
    - hostPath:
        path: /etc/cni/net.d
        type: DirectoryOrCreate
      name: etc-cni-netd
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - name: clustermesh-secrets
      secret:
        defaultMode: 256
        optional: true
        secretName: cilium-clustermesh
    - configMap:
        defaultMode: 420
        name: cilium-config
      name: cilium-config-path
    - hostPath:
        path: /proc/sys/net
        type: Directory
      name: host-proc-sys-net
    - hostPath:
        path: /proc/sys/kernel
        type: Directory
      name: host-proc-sys-kernel
    - name: hubble-tls
      projected:
        defaultMode: 256
        sources:
        - secret:
            items:
            - key: ca.crt
              path: client-ca.crt
            - key: tls.crt
              path: server.crt
            - key: tls.key
              path: server.key
            name: hubble-server-certs
            optional: true
    - name: kube-api-access-f6bjb
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-11-10T18:50:31Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-11-10T18:50:27Z"
      message: 'containers with unready status: [cilium-agent]'
      reason: ContainersNotReady
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-11-10T18:50:27Z"
      message: 'containers with unready status: [cilium-agent]'
      reason: ContainersNotReady
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-11-10T18:50:27Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://52f41d39cec5b2d5983d88a7d3e5286185fc7a28c839f0f873f40299a210cc3c
      image: sha256:1a370c63157a564daf7dc6e7dea585c783d960a01f12e6a63669a6f74cbff4da
      imageID: quay.io/cilium/cilium@sha256:30de50c4dc0a1e1077e9e7917a54d5cab253058b3f779822aec00f5c817ca826
      lastState:
        terminated:
          containerID: containerd://52f41d39cec5b2d5983d88a7d3e5286185fc7a28c839f0f873f40299a210cc3c
          exitCode: 1
          finishedAt: "2022-11-10T18:56:39Z"
          message: |
            |___|_|_|_|___|_|_|_|" subsys=daemon
            level=info msg="Cilium 1.12.3 1c466d2 2022-10-12T11:33:37+01:00 go version go1.18.6 linux/arm64" subsys=daemon
            level=info msg="cilium-envoy  version: 48cbb2f88edca93ba80d1135dbb067a827b8adc2/1.21.5/Distribution/RELEASE/BoringSSL" subsys=daemon
            level=info msg="clang (10.0.0) and kernel (5.15.74) versions: OK!" subsys=linux-datapath
            level=info msg="linking environment: OK!" subsys=linux-datapath
            level=info msg="Detected mounted BPF filesystem at /sys/fs/bpf" subsys=bpf
            level=info msg="Mounted cgroupv2 filesystem at /run/cilium/cgroupv2" subsys=cgroups
            level=info msg="Parsing base label prefixes from default label list" subsys=labels-filter
            level=info msg="Parsing additional label prefixes from user inputs: []" subsys=labels-filter
            level=info msg="Final label prefixes to be used for identity evaluation:" subsys=labels-filter
            level=info msg=" - reserved:.*" subsys=labels-filter
            level=info msg=" - :io\\.kubernetes\\.pod\\.namespace" subsys=labels-filter
            level=info msg=" - :io\\.cilium\\.k8s\\.namespace\\.labels" subsys=labels-filter
            level=info msg=" - :app\\.kubernetes\\.io" subsys=labels-filter
            level=info msg=" - !:io\\.kubernetes" subsys=labels-filter
            level=info msg=" - !:kubernetes\\.io" subsys=labels-filter
            level=info msg=" - !:.*beta\\.kubernetes\\.io" subsys=labels-filter
            level=info msg=" - !:k8s\\.io" subsys=labels-filter
            level=info msg=" - !:pod-template-generation" subsys=labels-filter
            level=info msg=" - !:pod-template-hash" subsys=labels-filter
            level=info msg=" - !:controller-revision-hash" subsys=labels-filter
            level=info msg=" - !:annotation.*" subsys=labels-filter
            level=info msg=" - !:etcd_node" subsys=labels-filter
            level=info msg="Auto-disabling \"enable-bpf-clock-probe\" feature since KERNEL_HZ cannot be determined" error="Cannot probe CONFIG_HZ" subsys=daemon
            level=info msg="Using autogenerated IPv4 allocation range" subsys=node v4Prefix=10.15.0.0/16
            level=fatal msg="install-no-conntrack-iptables-rules requires the agent to run in direct routing mode." subsys=daemon
          reason: Error
          startedAt: "2022-11-10T18:56:37Z"
      name: cilium-agent
      ready: false
      restartCount: 6
      started: false
      state:
        waiting:
          message: back-off 5m0s restarting failed container=cilium-agent pod=cilium-cbzlp_kube-system(f431a8c6-1f57-4db6-9679-472213eda73c)
          reason: CrashLoopBackOff
    ephemeralContainerStatuses:
    - image: quay.io/cilium/cilium:v1.12.3@sha256:30de50c4dc0a1e1077e9e7917a54d5cab253058b3f779822aec00f5c817ca826
      imageID: ""
      lastState: {}
      name: sysdump-1668106631
      ready: false
      restartCount: 0
      state:
        waiting:
          message: 'failed to generate container "6ffcad7723457cad96c988a5095df6a8332dcef0e59cc8021387ec1d479b4940"
            spec: invalid target container: container "52f41d39cec5b2d5983d88a7d3e5286185fc7a28c839f0f873f40299a210cc3c"
            is not running - in state CONTAINER_EXITED'
          reason: CreateContainerError
    hostIP: 192.168.5.15
    initContainerStatuses:
    - containerID: containerd://f1a76f395084c3a7f70ea49790dabdc7119f42399ae7f7eb933f7c4d9202427d
      image: sha256:1a370c63157a564daf7dc6e7dea585c783d960a01f12e6a63669a6f74cbff4da
      imageID: quay.io/cilium/cilium@sha256:30de50c4dc0a1e1077e9e7917a54d5cab253058b3f779822aec00f5c817ca826
      lastState: {}
      name: mount-cgroup
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://f1a76f395084c3a7f70ea49790dabdc7119f42399ae7f7eb933f7c4d9202427d
          exitCode: 0
          finishedAt: "2022-11-10T18:50:28Z"
          reason: Completed
          startedAt: "2022-11-10T18:50:28Z"
    - containerID: containerd://8798abb87f6c81965c9c3916e1365b00dad588f2eb31abac0612e26ab96b76c0
      image: sha256:1a370c63157a564daf7dc6e7dea585c783d960a01f12e6a63669a6f74cbff4da
      imageID: quay.io/cilium/cilium@sha256:30de50c4dc0a1e1077e9e7917a54d5cab253058b3f779822aec00f5c817ca826
      lastState: {}
      name: apply-sysctl-overwrites
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://8798abb87f6c81965c9c3916e1365b00dad588f2eb31abac0612e26ab96b76c0
          exitCode: 0
          finishedAt: "2022-11-10T18:50:28Z"
          reason: Completed
          startedAt: "2022-11-10T18:50:28Z"
    - containerID: containerd://b7a8f4c64a0d7d33de699770cb847a521604fc76f165c81b82566b768a2123d0
      image: sha256:1a370c63157a564daf7dc6e7dea585c783d960a01f12e6a63669a6f74cbff4da
      imageID: quay.io/cilium/cilium@sha256:30de50c4dc0a1e1077e9e7917a54d5cab253058b3f779822aec00f5c817ca826
      lastState: {}
      name: mount-bpf-fs
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://b7a8f4c64a0d7d33de699770cb847a521604fc76f165c81b82566b768a2123d0
          exitCode: 0
          finishedAt: "2022-11-10T18:50:29Z"
          reason: Completed
          startedAt: "2022-11-10T18:50:29Z"
    - containerID: containerd://8900d83465c693fe2e80b3688d80b9f61a4591fc38fde8a5b33aadc72458ce0c
      image: sha256:1a370c63157a564daf7dc6e7dea585c783d960a01f12e6a63669a6f74cbff4da
      imageID: quay.io/cilium/cilium@sha256:30de50c4dc0a1e1077e9e7917a54d5cab253058b3f779822aec00f5c817ca826
      lastState: {}
      name: clean-cilium-state
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://8900d83465c693fe2e80b3688d80b9f61a4591fc38fde8a5b33aadc72458ce0c
          exitCode: 0
          finishedAt: "2022-11-10T18:50:30Z"
          reason: Completed
          startedAt: "2022-11-10T18:50:30Z"
    phase: Running
    podIP: 192.168.5.15
    podIPs:
    - ip: 192.168.5.15
    - ip: fec0::5055:55ff:fed4:b233
    qosClass: Burstable
    startTime: "2022-11-10T18:50:27Z"
kind: PodList
metadata:
  resourceVersion: "1884"
